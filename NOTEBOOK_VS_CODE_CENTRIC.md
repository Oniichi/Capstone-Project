# Why Create src/ (Or Not): Two Valid Approaches

## The Fundamental Question
**"Why extract code to src/ if I can just automate running notebooks?"**

Great question. The answer is: **It depends on your goal.**

---

## Approach 1: "Notebook-Centric" (What You're Doing)

### Structure
```
notebooks/
â”œâ”€â”€ 01_data_collection.ipynb
â”œâ”€â”€ 02_data_cleaning.ipynb
â”œâ”€â”€ 03_eda.ipynb
â”œâ”€â”€ 04_feature_engineering.ipynb
â”œâ”€â”€ 05_baseline_models.ipynb
â”œâ”€â”€ 06_ml_models.ipynb
â”œâ”€â”€ 07_deep_learning.ipynb
â”œâ”€â”€ 08_walk_forward_validation.ipynb
â””â”€â”€ 09_final_summary.ipynb

scripts/
â””â”€â”€ run_all_notebooks.py  # Just executes notebooks in order
```

### Pipeline: Run All Notebooks Automatically
```python
# scripts/run_all_notebooks.py
import papermill as pm

notebooks = [
    'notebooks/01_data_collection.ipynb',
    'notebooks/02_data_cleaning.ipynb',
    'notebooks/03_eda.ipynb',
    'notebooks/04_feature_engineering.ipynb',
    'notebooks/05_baseline_models.ipynb',
    'notebooks/06_ml_models.ipynb',
    'notebooks/07_deep_learning.ipynb',
    'notebooks/08_walk_forward_validation.ipynb',
    'notebooks/09_final_summary.ipynb',
]

for nb in notebooks:
    print(f"\nRunning {nb}...")
    pm.execute_notebook(nb, nb)  # Runs notebook, saves results
    
print("\nâœ… All notebooks executed!")
```

### Usage
```bash
python scripts/run_all_notebooks.py
# Automatically runs all 9 notebooks in order
# Done!
```

### Pros âœ…
- **Simple** - No code extraction needed
- **Works for capstone** - Notebooks show your thinking
- **Transparent** - All analysis visible in notebooks
- **Good for exploration** - Notebooks = exploratory work
- **Fast to set up** - Just one pipeline script

### Cons âŒ
- **Hard to reuse** - If you want model for production, copy-paste from notebook
- **Code duplication** - Same function in multiple notebooks
- **Slow to change** - Modify model hyperparameter â†’ Re-run 8 notebooks
- **Not production-ready** - Can't import functions from notebooks easily
- **Hard to test** - Can't unit test notebook cells

### When to Use This âœ…
- **Academic/capstone projects** (like yours!)
- **One-time analysis**
- **Exploratory work**
- **When notebooks = final deliverable**

---

## Approach 2: "Code-Centric" (Production-Grade)

### Structure
```
src/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ loaders.py
â”‚   â””â”€â”€ cleaners.py
â”œâ”€â”€ features/
â”‚   â””â”€â”€ engineering.py
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ baselines.py
â”‚   â”œâ”€â”€ ml.py
â”‚   â””â”€â”€ dl.py
â”œâ”€â”€ evaluation/
â”‚   â””â”€â”€ metrics.py
â””â”€â”€ pipeline/
    â””â”€â”€ orchestrator.py

notebooks/
â”œâ”€â”€ 01_data_collection.ipynb
â”œâ”€â”€ 02_data_cleaning.ipynb
â””â”€â”€ ... (exploratory only)

scripts/
â””â”€â”€ train.py
```

### Pipeline: Run Modular Functions
```python
# src/pipeline/orchestrator.py
from src.data.loaders import load_data
from src.data.cleaners import clean_data
from src.features.engineering import engineer_features
from src.models.baselines import train_baselines
from src.models.ml import train_ml_models
from src.models.dl import train_dl_models
from src.evaluation.metrics import compute_metrics

class Pipeline:
    def run(self):
        df = load_data()
        df = clean_data(df)
        df = engineer_features(df)
        
        baseline_results = train_baselines(df)
        ml_results = train_ml_models(df)
        dl_results = train_dl_models(df)
        
        return {
            'baselines': baseline_results,
            'ml': ml_results,
            'dl': dl_results
        }

# scripts/train.py
if __name__ == '__main__':
    pipeline = Pipeline()
    results = pipeline.run()
```

### Usage
```bash
python scripts/train.py
# Runs modular functions from src/
```

### Pros âœ…
- **Reusable** - Import functions: `from src.models.ml import train_rf`
- **Modular** - Easy to change one piece
- **Fast iterations** - Change hyperparameter, run only affected step
- **Production-ready** - Can deploy as package
- **Testable** - Write unit tests for functions
- **Professional** - Industry-standard structure

### Cons âŒ
- **More setup** - Need to extract code from notebooks
- **Overhead** - More files to maintain
- **Less transparent** - Logic split across multiple files
- **Overkill for capstone** - More than usually needed

### When to Use This âœ…
- **Production systems**
- **Building a package/library**
- **Long-term maintenance**
- **Team projects**
- **When code will be reused**

---

## Comparison Table

| Aspect | Notebook-Centric | Code-Centric |
|--------|------------------|--------------|
| **Setup time** | 1 hour | 4 hours |
| **Run all** | `python scripts/run_all_notebooks.py` | `python scripts/train.py` |
| **Change hyperparameter** | Edit notebook, re-run all 9 | Edit config, re-run pipeline |
| **Add new model** | Add to notebook 06 | Add to src/models/, auto-integrated |
| **Reuse code elsewhere** | Copy-paste | `from src.models import train_rf` |
| **Test code** | Notebooks (hard) | Unit tests (easy) |
| **Production deployment** | âŒ Can't deploy | âœ… Can deploy |
| **For capstone** | âœ… Perfect | âŒ Overkill |
| **Transparency** | âœ… Show all thinking | âŒ Logic hidden in modules |
| **Professional** | âš ï¸ Semi-pro | âœ… Pro |

---

## My Recommendation for YOUR Project

### **Use Approach 1 (Notebook-Centric)**

Why?
1. **Capstone = show your work** - Notebooks demonstrate thinking
2. **You have 8 great notebooks** - Why refactor?
3. **Pipeline automation is enough** - Just automate running them
4. **Less work** - Focus on results, not structure
5. **Professors like notebooks** - Shows methodology clearly

### Simple Implementation

```python
# scripts/run_all_notebooks.py
import papermill as pm
from pathlib import Path

notebooks = [
    'notebooks/01_data_collection.ipynb',
    'notebooks/02_data_cleaning.ipynb',
    'notebooks/03_eda.ipynb',
    'notebooks/04_feature_engineering.ipynb',
    'notebooks/05_baseline_models.ipynb',
    'notebooks/06_ml_models.ipynb',
    'notebooks/07_deep_learning.ipynb',
    'notebooks/08_walk_forward_validation.ipynb',
    'notebooks/09_final_summary.ipynb',
]

print("ğŸš€ Running full analysis pipeline...\n")

for i, nb in enumerate(notebooks, 1):
    print(f"[{i}/9] Running {Path(nb).stem}...")
    try:
        pm.execute_notebook(nb, nb, kernel_name='python3')
        print(f"      âœ… Complete")
    except Exception as e:
        print(f"      âŒ Error: {e}")
        break

print("\nâœ… Pipeline complete! All results saved.")
```

### Usage
```bash
pip install papermill
python scripts/run_all_notebooks.py
# Runs all notebooks automatically
# All results saved
```

---

## Installation (One-Time)
```bash
pip install papermill
```

---

## What This Gives You

âœ… **Single command** - `python scripts/run_all_notebooks.py`
âœ… **Full automation** - No manual clicking
âœ… **Easy reproduction** - Anyone can re-run
âœ… **Keeps notebooks** - All analysis visible
âœ… **5 minutes to implement** - Minimal extra work

---

## When to Use src/

Only if:
- [ ] You need production deployment
- [ ] You want to reuse code in other projects
- [ ] You need unit tests
- [ ] You're building a package
- [ ] You have a team

For a **capstone project**, these are rarely needed.

---

## Summary

| Question | Answer |
|----------|--------|
| **Do I need to refactor to src/?** | No, optional |
| **Can I just automate notebooks?** | Yes, perfect! |
| **How do I automate notebooks?** | Use `papermill` |
| **Is this professional enough?** | Yes, for capstone |
| **Will my professor like it?** | Yes, shows your work |
| **Time investment** | 5 minutes |

**Bottom line**: Automate your notebooks with `papermill`. Keep everything transparent. That's enough for a capstone! ğŸ¯
